{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55919b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import statistics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, LeaveOneOut, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, RationalQuadratic, Matern\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66522bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(X_data, y_data, n_samples, noise):\n",
    "   '''\n",
    "   Bootstrap data by resamping from dataset to a size of n_samples and adding Gaussian noise to the y data\n",
    "   Inputs: X_data = array of feature vectors, y_data = array of outputs, n_samples = Desired final data size, \n",
    "   noise = Standard deviation of Gaussian distribution\n",
    "   \n",
    "    Outputs: resampled list of feature vectors and resampled list of column densities \n",
    "    '''\n",
    "    X_resample, y_resample = resample(X_data, y_data, n_samples=n_samples) # Resample n samples from X and y data\n",
    "    noise_array = np.random.normal(0., noise, size=y_resample.size) # Sample from Gaussian distribution with mean 0 and standard deviation = noise n times\n",
    "    y_resample = y_resample + noise_array # Add Gaussian noise to y data\n",
    "    return X_resample, y_resample\n",
    "\n",
    "def scaleData(X_train,X_test,X_tot):\n",
    "    '''\n",
    "    Standardizes feature vectors so tha the individual features more\n",
    "    or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "    '''\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train) #creating Standard Scaler Object \n",
    "    X_train_scaled = scaler.transform(X_train) #Scaling X_train\n",
    "    X_test_scaled = scaler.transform(X_test) #Scaling X_test\n",
    "    X_tot_scaled = scaler.transform(X_tot)\n",
    "    return X_train_scaled, X_test_scaled, X_tot_scaled\n",
    "\n",
    "def splitBootScale(x,y,bootSize = 800,trainSize = 0.8):\n",
    "    \n",
    "    '''\n",
    "    Splits data 80/20 into train and test sets, bootstraps to a total of 800 samples and scales\n",
    "    the resulting feature vectors\n",
    "    \n",
    "    '''\n",
    "    testSize = 1-trainSize\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, train_size = trainSize, test_size = testSize, random_state=85)\n",
    "    print(len(X_train), len(X_test))\n",
    "    bootTrainSize = round(bootSize*trainSize)\n",
    "    print(bootTrainSize)\n",
    "    bootTestSize = round(bootSize*testSize)\n",
    "    print(bootTestSize)\n",
    "    X_train_boot, y_train_boot = resampling(X_train, y_train, bootTrainSize, 0.5)\n",
    "    X_test_boot, y_test_boot = resampling(X_test, y_test, bootTestSize, 0.5)\n",
    "    X_train_bootScaled, X_test_bootScaled, x_scaled = scaleData(X_train_boot,X_test_boot,x)\n",
    "    return X_train_bootScaled, X_test_bootScaled, y_train_boot, y_test_boot, x_scaled\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Three functions that are necessary to convert lists into strings and vice versa. For example,\n",
    "converting the list [1,2,3] to the string \"[1,2,3]\". This is necessary in order to legibly store long\n",
    "vectors in a .csv final using the Pandas module. \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def str2float(string):\n",
    "    split = list(string.split(','))\n",
    "    floats_split = []\n",
    "    for i in range(len(split)):\n",
    "        floats = float(split[i])\n",
    "        floats_split.append(floats)\n",
    "    return floats_split\n",
    "\n",
    "def stringToList(vectors):\n",
    "    bracket_removed_mol2vec = []\n",
    "    for i in range(len(vectors)):\n",
    "        new_strings = vectors[i].replace('[', '')\n",
    "        newer_strings = new_strings.replace(']', '')\n",
    "        bracket_removed_mol2vec.append(newer_strings)\n",
    "\n",
    "    xList = []\n",
    "    for i in range(len(bracket_removed_mol2vec)):\n",
    "        float_vec = str2float(bracket_removed_mol2vec[i])\n",
    "        xList.append(float_vec)\n",
    "    \n",
    "    return xList\n",
    "\n",
    "def listToString(vectors):\n",
    "    string_indices = []\n",
    "    for i in range(len(vectors)):\n",
    "        knn_string = ', '.join(str(k) for k in vectors[i])\n",
    "        string_indices.append(knn_string)\n",
    "\n",
    "    bracket_string_indices = []\n",
    "    for i in range(len(string_indices)):\n",
    "        bracket_string = '[' + string_indices[i] + ']'\n",
    "        bracket_string_indices.append(bracket_string)\n",
    "    \n",
    "    return bracket_string_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uploads information about detected species\n",
    "\n",
    "Inputs: None\n",
    "Outputs: List of feature vectors, log 10 column densities, column densities and SMILES strings\n",
    "of each detected molecule\n",
    "'''\n",
    "\n",
    "def uploadIso():\n",
    "    fullPath = os.path.join(os.getcwd(), 'detectionDataset.csv')\n",
    "    fullUpload = pd.read_csv(fullPath)\n",
    "    \n",
    "    mol2vec_strings = list(fullUpload['mol2vec'])\n",
    "    detectedSmiles = list(fullUpload['smiles'])\n",
    "    xList = stringToList(mol2vec_strings)\n",
    "    cd = np.asarray(newDataset['N'])\n",
    "    cdLog = np.log10(cd)\n",
    "    \n",
    "    return xList, cdLog, cd, detectedSmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9764a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that performs K Means Clustering on the molecules in the dataset and returns the molecules \n",
    "that are in the cluster that contains the most detected species. \n",
    "\n",
    "Inputs: Feature vectors and corresponding smiles strings of entire dataset\n",
    "Returns: Feature vectors and corresponding smiles strings of the molecules that are in the cluster\n",
    "that contains the most detected species. Also saves these resulting molecules and vectors\n",
    "in a file called commonCluster.csv\n",
    "'''\n",
    "\n",
    "\n",
    "def runClustering(smiles, vectors)\n",
    "    \n",
    "    clusterModel = KMeans(n_clusters=10)\n",
    "    clusterResult = clusterModel.fit(vectors)\n",
    "    clusterLabels = clusterResult.labels_\n",
    "    \n",
    "    fullPath = os.path.join(os.getcwd(), 'detectionDataset.csv')\n",
    "    fullUpload = pd.read_csv(fullPath)\n",
    "\n",
    "    detectSmiles = list(fullUpload['smiles'])\n",
    "\n",
    "    clusterDetection = []\n",
    "\n",
    "    for i in detectSmiles:\n",
    "        idx = smiles.index(i)\n",
    "        clusterDetection.append(clusterLabels[idx])\n",
    "    \n",
    "    mostCommonCluster = statistics.mode(clusterDetection)\n",
    "    \n",
    "    clusterVectors = []\n",
    "    clusterSmiles = []\n",
    "    for i in range(len(clusterLabels)):\n",
    "        if clusterLabels[i] == mostCommonCluster:\n",
    "            clusterVectors.append(vectors[i])\n",
    "            clusterSmiles.append(smiles[i])\n",
    "            \n",
    "    clusterDF = pd.DataFrame()\n",
    "    clusterDF['smiles'] = clusterSmiles\n",
    "    clusterDF['mol2vec'] = clusterVectors\n",
    "    \n",
    "    savePath = os.path.join(os.getcwd(), 'commonCluster.csv')\n",
    "    clusterDF.to_csv(savePath)\n",
    "    \n",
    "    return clusterSmiles, clusterVectors\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates the nearest neighbors to the detected molecules\n",
    "\n",
    "Inputs: number of nearest neighbors to include\n",
    "Returns: List of triple of all nearest neighbors to the detected species that includes the smiles string\n",
    "of each molecule, the distance to the nearest detected species and the index of the species. Also saves\n",
    "the nearest neighbor SMILES strings and feature vectors in a file called closestNeighbors.csv\n",
    "\n",
    "'''\n",
    "\n",
    "def getClosestNeighbors(n_neighbors):\n",
    "    \n",
    "    completeList = []\n",
    "    idxList = []\n",
    "    \n",
    "    clusterPath = os.path.join(os.getcwd(), 'commonCluster.csv')\n",
    "    clusters = pd.read_csv(clusterPath)\n",
    "    clusterSmiles = list(clusters['smiles'])\n",
    "    clusterStrings = list(clusters['mol2vec'])\n",
    "    clusterVectors = stringToList(clusterStrings)\n",
    "    \n",
    "    detectVectors, cdLog, cd, detectionSmiles = uploadIso()\n",
    "    \n",
    "\n",
    "    for i in range(len(detectionSmiles)):\n",
    "        print(i)\n",
    "        individualList = []\n",
    "        testMol = np.asarray(clusterVectors[i])\n",
    "        \n",
    "        for j in range(len(clusterSmiles)):\n",
    "            clusterMol = np.asarray(clusterVectors[j])\n",
    "            clusterSmile = clusterStrings[j]\n",
    "            distance = np.linalg.norm(testMol-clusterMol)\n",
    "            tu = (clusterSmile, distance, j)\n",
    "            individualList.append(tu)\n",
    "        \n",
    "        individualList.sort(key=lambda y: y[1])\n",
    "        filteredList = individualList[0:n_neighbors]\n",
    "        \n",
    "        completeList.append(filteredList)\n",
    "        \n",
    "        \n",
    "    for subList in completeList:\n",
    "        for tup in subList:\n",
    "            idxList.append(tup[2])\n",
    "    \n",
    "    deleteDuplicates(idxList)\n",
    "    \n",
    "    closestDF = clusters.iloc[idxList]\n",
    "    \n",
    "    savePath = os.path.join(os.getcwd(), 'closestNeighbors.csv')\n",
    "    closestDF.to_csv(savePath)\n",
    "    \n",
    "    return completeList     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to delete multiple indices from a list\n",
    "retrieved from https://thispointer.com/python-remove-elements-from-list-by-index/\n",
    "'''\n",
    "def deleteMultiple(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Gaussian Process Regression on all detected species then returning column density\n",
    "predictions on a list of new species\n",
    "\n",
    "Inputs: feature vectors and SMILES strings of new molecules t\n",
    "Outputs: A triple containing the column density prediction information with the format:\n",
    "\n",
    "(SMILES string, predicted column density, 1 sigma uncertainty of column density prediction)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def newPredictions(X_test, testSmiles, bootSize = 800):\n",
    "    xDetect, cdLog, cd, detectedSmiles = uploadIso()\n",
    "    removeIdx = []\n",
    "    for i in range(len(testSmiles)):\n",
    "        if testSmiles[i] in detectedSmiles:\n",
    "            removeIdx.append(i)\n",
    "    deleteMultiple(testSmiles, removeIdx)\n",
    "    deleteMultiple(X_test, removeIdx)\n",
    "    X_train, y_train = resampling(xDetect, cdLog, bootSize, 0.5)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    #BR_best_params = {'model__alpha_1': 100000.0, 'model__alpha_2': 100000.0, 'model__alpha_init': 100.0, 'model__lambda_1': 100000.0, 'model__lambda_2': 100000.0, 'model__lambda_init': 100.0, 'model__tol': 1e-07}\n",
    "    model = BayesianRidge()\n",
    "    result = model.fit(X_train, y_train)\n",
    "    validationResults, valSD = result.predict(X_test_scaled, return_std = True)\n",
    "    \n",
    "    completeList = [(testSmiles[i], validationResults[i], valSD[i]) for i in range(len(testSmiles))]\n",
    "    completeList.sort(key = lambda x: x[1])\n",
    "    \n",
    "    return completeList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
