{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df319485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing mol2vec\n",
    "!pip install git+https://github.com/samoturk/mol2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required packages\n",
    "\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile\n",
    "import fileinput\n",
    "import os\n",
    "import rdkit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mol2vec\n",
    "from mol2vec import features\n",
    "from mol2vec import helpers\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "import pkg_resources\n",
    "#pkg_resources.require(\"gensim==3.8.3\")  \n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from mol2vec.helpers import depict_identifier, plot_2D_vectors, IdentifierTable, mol_to_svg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "from rdkit import RDLogger   \n",
    "RDLogger.DisableLog('rdApp.*') # turn off RDKit warning message \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These two functions are copied from the mol2Vec GitHub repository. \n",
    "It was needed to manually copy them and edit them in the notebook due to \n",
    "package version updates of several modules.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def train_word2vec_model(infile_name, outfile_name=None, vector_size=100, window=10, min_count=3, n_jobs=1,\n",
    "                         method='skip-gram', **kwargs):\n",
    "    \"\"\"Trains word2vec (Mol2vec, ProtVec) model on corpus file extracted from molecule/protein sequences.\n",
    "    The corpus file is treated as LineSentence corpus (one sentence = one line, words separated by whitespaces)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    infile_name : str\n",
    "        Corpus file, e.g. proteins split in n-grams or compound identifier\n",
    "    outfile_name : str\n",
    "        Name of output file where word2vec model should be saved\n",
    "    vector_size : int\n",
    "        Number of dimensions of vector\n",
    "    window : int\n",
    "        Number of words considered as context\n",
    "    min_count : int\n",
    "        Number of occurrences a word should have to be considered in training\n",
    "    n_jobs : int\n",
    "        Number of cpu cores used for calculation\n",
    "    method : str\n",
    "        Method to use in model training. Options cbow and skip-gram, default: skip-gram)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word2vec.Word2Vec\n",
    "    \"\"\"\n",
    "    if method.lower() == 'skip-gram':\n",
    "        sg = 1\n",
    "    elif method.lower() == 'cbow':\n",
    "        sg = 0\n",
    "    else:\n",
    "        raise ValueError('skip-gram or cbow are only valid options')\n",
    "  \n",
    "    start = timeit.default_timer()\n",
    "    corpus = word2vec.LineSentence(infile_name)\n",
    "    model = word2vec.Word2Vec(corpus, size=vector_size, window=window, min_count=min_count, workers=n_jobs, sg=sg,\n",
    "                              **kwargs)\n",
    "    if outfile_name:\n",
    "        model.save(outfile_name)\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    print('Runtime: ', round((stop - start)/60, 2), ' minutes')\n",
    "    return model\n",
    "\n",
    "\n",
    "def sentences2vec(sentences, model, unseen=None):\n",
    "    \"\"\"Generate vectors for each sentence (list) in a list of sentences. Vector is simply a\n",
    "    sum of vectors for individual words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list, array\n",
    "        List with sentences\n",
    "    model : word2vec.Word2Vec\n",
    "        Gensim word2vec model\n",
    "    unseen : None, str\n",
    "        Keyword for unseen words. If None, those words are skipped.\n",
    "        https://stats.stackexchange.com/questions/163005/how-to-set-the-dictionary-for-text-analysis-using-neural-networks/163032#163032\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "    \"\"\"\n",
    "    keys = set(model.wv.vocab.keys())\n",
    "    #keys = set(model.wv.index_to_key)\n",
    "    vec = []\n",
    "    if unseen:\n",
    "        unseen_vec = model.wv.word_vec(unseen)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if unseen:\n",
    "            vec.append(sum([model.wv.word_vec(y) if y in set(sentence) & keys\n",
    "                       else unseen_vec for y in sentence]))\n",
    "        else:\n",
    "            vec.append(sum([model.wv.word_vec(y) for y in sentence \n",
    "                            if y in set(sentence) & keys]))\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ccea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Three functions that assist in converting lists to strings and vice versa.\n",
    "These are required in order to store vectors in csv files. \n",
    "'''\n",
    "\n",
    "\n",
    "def str2float(string):\n",
    "    split = list(string.split(','))\n",
    "    floats_split = []\n",
    "    for i in range(len(split)):\n",
    "        floats = float(split[i])\n",
    "        floats_split.append(floats)\n",
    "    return floats_split\n",
    "\n",
    "def stringToList(vectors):\n",
    "    bracket_removed_mol2vec = []\n",
    "    for i in range(len(vectors)):\n",
    "        new_strings = vectors[i].replace('[', '')\n",
    "        newer_strings = new_strings.replace(']', '')\n",
    "        bracket_removed_mol2vec.append(newer_strings)\n",
    "\n",
    "# Convert all vectors\n",
    "    xList = []\n",
    "    for i in range(len(bracket_removed_mol2vec)):\n",
    "        float_vec = str2float(bracket_removed_mol2vec[i])\n",
    "        xList.append(float_vec)\n",
    "    \n",
    "    return xList\n",
    "    \n",
    "def listToString(vectors):\n",
    "    string_indices = []\n",
    "    for i in range(len(vectors)):\n",
    "        knn_string = ', '.join(str(k) for k in vectors[i])\n",
    "        string_indices.append(knn_string)\n",
    "\n",
    "    bracket_string_indices = []\n",
    "    for i in range(len(string_indices)):\n",
    "        bracket_string = '[' + string_indices[i] + ']'\n",
    "        bracket_string_indices.append(bracket_string)\n",
    "    \n",
    "    return bracket_string_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing all SMILES strings in dataset\n",
    "'''\n",
    "\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'fullDataset.csv')\n",
    "full = pd.read_csv(path)\n",
    "smile = list(full['smile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Writing all SMILES string into a .smi file\n",
    "'''\n",
    "\n",
    "fileName = 'allSmiles.smi'\n",
    "filePath = os.path.join(os.getcwd(), fileName)\n",
    "with open(filePath, 'w') as f:\n",
    "    for smi in smile:\n",
    "        f.write(\"{}\\n\".format(smi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generating substructure corpus that is used to train word2Vec model on the molecular dataset.\n",
    "This takes around 20 minutes.\n",
    "\n",
    "'''\n",
    "CORPUSNAME = \"mol2vec_corpus.dat\"\n",
    "RADIUS = 1\n",
    "NJOBS = 32\n",
    "\n",
    "fileName = 'allSmiles.smi'\n",
    "filePath = os.path.join(os.getcwd(), fileName)\n",
    "\n",
    "features.generate_corpus(\n",
    "    filePath,\n",
    "    CORPUSNAME,\n",
    "    RADIUS,\n",
    "    sentence_type=\"alt\",\n",
    "    n_jobs=NJOBS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c55d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training word2Vec model on substructure corpus. \n",
    "This trained model can then be used to create length 70 molecular feature vectors.\n",
    "'''\n",
    "\n",
    "modelName = 'mol2vec_model_final_70.pkl'\n",
    "modelPath = os.path.join(os.getcwd(), modelName)\n",
    "\n",
    "model = features.train_word2vec_model(corpusPath, modelPath, vector_size = 70, min_count=1, n_jobs=NJOBS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-rdkit-env] *",
   "language": "python",
   "name": "conda-env-my-rdkit-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
